##  Multi-Class Classification of Disaster-Related Tweets Using DistilBERT for Real-Time Emergency Response

###  Intermediate Update 1 â€” Model Implementation & Initial Training

---

### âœ… Work Completed

* Loaded the Kaggle dataset: *Natural Language Processing with Disaster Tweets*
* Cleaned missing values in **keyword** and **location** columns
* Performed Exploratory Data Analysis (EDA) on tweet distribution and length
* Implemented two models:

  * **TF-IDF + Logistic Regression** â†’ Baseline
  * **DistilBERT** (fine-tuned transformer, 2 epochs)
* Collected evaluation metrics, generated plots, and saved organized outputs

---

### ðŸ“Š EDA Plots

| Plot                | Description                                         |
| :------------------ | :-------------------------------------------------- |
| `class_balance.png` | Class distribution (0 = Non-Disaster, 1 = Disaster) |
| `tweet_length.png`  | Tweet-length histogram                              |

---

###  Model Performance

| Model                        | Accuracy | F1 Score |
| :--------------------------- | :------: | :------: |
| TF-IDF + Logistic Regression |  0.8070  |  0.7637  |
| DistilBERT (2 epochs)        |  0.8437  |  0.8128  |

---

###  Evaluation Figures

| File                             | Description                                          |
| :------------------------------- | :--------------------------------------------------- |
| `baseline_cm.png`                | Confusion matrix for baseline model                  |
| `loss_curve_distilbert.png`      | Training/Eval loss curve for DistilBERT              |

---

###  Metrics Files

| File             | Purpose                               |
| :--------------- | :------------------------------------ |
| `baseline.csv`   | Accuracy & F1 for Logistic Regression |
| `distilbert.csv` | Accuracy & F1 for DistilBERT model    |

---

###  Folder Layout

```
ml-m1-disaster-tweets/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ train.csv
â”‚       â”œâ”€â”€ test.csv
â”‚       â””â”€â”€ sample_submission.csv
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ figs/
â”‚   â”‚   â”œâ”€â”€ class_balance.png
â”‚   â”‚   â”œâ”€â”€ tweet_length.png
â”‚   â”‚   â”œâ”€â”€ baseline_cm.png
â”‚   â”‚   â”œâ”€â”€ loss_curve_distilbert.png
â”‚   â”‚
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ baseline.csv
â”‚   â”‚   â””â”€â”€ distilbert.csv
â”‚   â”‚
â”‚   â””â”€â”€ checkpoints/            # saved model weights
â”‚
â””â”€â”€ README.md
```

---

###  Challenges Faced

* Limited Colab GPU time â†’ DistilBERT training stopped at 2 epochs
* Tweets contained hashtags, links & emojis â†’ needed extra pre-processing
* Large model files excluded from GitHub via `.gitignore`
* Transformer training was computationally expensive

---

### ðŸ”œ Next Steps

* Extend DistilBERT training to 3â€“5 epochs for better accuracy
* Apply advanced text normalization (remove URLs, mentions, emojis)
* Experiment with other transformer models (RoBERTa, BERTweet)
* Prepare final report and presentation slides

---

### ðŸ“‰ Training Curve

* `loss_curve_distilbert.png` â†’ Shows training vs evaluation loss trend for DistilBERT

---

âœ… **Status:** All deliverables for Intermediate Update 1 are completed and pushed to GitHub â€” dataset, EDA, baseline model, transformer model, metrics, and visualizations ready for review.

---
